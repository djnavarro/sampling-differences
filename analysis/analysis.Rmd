---
title: "Analysis"
author: "Danielle Navarro"
date: "30-Jun-2021"
output:
  rmarkdown::html_document:
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source(here::here("models", "sampling_frames_model.R"))
`%>%` <- magrittr::`%>%`
```

## Aggregate level

To begin, let's import the data and examine it at an aggregate level. Here's the data:

```{r import-data, message=FALSE}
import_data <- function(experiment) {
  ci_quantile <- function(x, quantile) {
    mean(x) + qt(quantile, length(x) - 1) * sd(x) / sqrt(length(x) - 1)
  }
  here::here("data", paste0(experiment, ".csv")) %>% 
    readr::read_csv() %>% 
    dplyr::group_by(sample_size, sampling_frame, test_item) %>% 
    dplyr::summarise(
      ci_lower = ci_quantile(response / 10, .025),
      ci_upper = ci_quantile(response / 10, .975),
      response = mean(response / 10)
    ) %>% 
    dplyr::ungroup() %>% 
    dplyr::mutate(source = "human", exp = experiment)
}

human_aggregate <- dplyr::bind_rows(import_data("exp1"), import_data("exp2"))
human_aggregate
```

Here is a simple visualisation that plots the mean and 95% confidence interval for the average response in every condition in and both experiments. (The confidence intervals here are Student intervals calculated independently for each condition)

```{r data-visualisation}
ggplot2::ggplot(
  data = human_aggregate %>% dplyr::mutate(
    exp = exp %>% stringr::str_replace_all("exp", "Experiment "),
    sampling_frame = sampling_frame %>% stringr::str_to_sentence()
  ), 
  mapping = ggplot2::aes(
    x = test_item, 
    y = response,
    colour = factor(sample_size),
    group = sample_size
  )
) +
  ggplot2::facet_grid(
    rows = ggplot2::vars(exp),
    cols = ggplot2::vars(sampling_frame)
  ) + 
  ggplot2::geom_path(
    show.legend = FALSE,
    size = .25,
    linetype = "dashed",
    position = ggplot2::position_dodge2(
      width = 0.5, 
      padding = 0.5
    )
  ) + 
  ggplot2::geom_pointrange(
    mapping = ggplot2::aes(
      ymin = ci_lower,
      ymax = ci_upper,
    ),
    position = ggplot2::position_dodge2(
      width = 0.5, 
      padding = 0.5
    )
  ) +
  ggplot2::theme_bw() + 
  ggplot2::coord_cartesian(ylim = c(0, 1)) + 
  ggplot2::scale_x_continuous(
    name = "Test stimulus",
    breaks = 1:6, 
    minor_breaks = NULL
  ) + 
  ggplot2::scale_y_continuous(
    name = "Response",
    minor_breaks = NULL
  ) + 
  ggplot2::scale_color_brewer(
    name = "Sample size", 
    palette = "Dark2"
  )
```

This visualisation oversimplifies a little as it doesn't consider systematic individual differences and reports only aggregated data, but it is helpful for illustrating what the overall pattern looks like. As a point of comparison, here is an example of the kind of behaviour one might expect from the model. In this example we use the same parameter values reported by Hayes et al (2019), though not for any principled reason because their designs were somewhat different to ours. However, those parameter settings ($\tau = 1.5$, $\rho = .1$, $\sigma = .5$ and $\mu = .5$) make a convenient starting point, and as demonstrated by the simulations reported by Hayes et al, the key qualitative predictions of the model are almost invariant across the parameter space: 

```{r model-visualisation, fig.height=3}
model_plot <- function(model) {
  ggplot2::ggplot(
    data = model %>% dplyr::mutate(
      sampling_frame = sampling_frame %>% stringr::str_to_sentence()
    ), 
    mapping = ggplot2::aes(
      x = test_item, 
      y = response,
      colour = factor(sample_size),
      group = sample_size
    )
  ) +
    ggplot2::facet_grid(
      cols = ggplot2::vars(sampling_frame)
    ) + 
    ggplot2::geom_path(
      show.legend = FALSE,
      size = .25,
      linetype = "dashed",
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      )
    ) + 
    ggplot2::geom_point(
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      ),
      size = 3
    ) +
    ggplot2::theme_bw() + 
    ggplot2::coord_cartesian(ylim = c(0, 1)) + 
    ggplot2::scale_x_continuous(
      name = "Test stimulus",
      breaks = 1:6, 
      minor_breaks = NULL
    ) + 
    ggplot2::scale_y_continuous(
      name = "Response",
      minor_breaks = NULL
    ) + 
    ggplot2::scale_color_brewer(
      name = "Sample size", 
      palette = "Dark2"
    )
}

sampling_frames_model(tau = 1.5, rho = .1, sigma = .5, mu = .5) %>% 
  model_plot()
```

What matters for the moment is that the model behaviour is (qualitatively speaking) quite similar to the aggregate pattern of results produced by human participants, replicating the findings of Hayes et al (2019). Some aspects to this pattern are not theoretically interesting, however. For example, in both the property and category sampling conditions there is a tendency for generalisation responses to diminish as a function of distance (left to right in the plots). This pattern is of course captured by the model, but this is not diagnostic of anything: the relationship between similarity and generalisation is one of the oldest and most consistent effects in the literature. The fact that our data and model both reproduce it is a small sanity check that our data are not garbage, but it is otherwise of no interest to us. 

In terms of the qualitative patterns that *are* relevant to the sampling manipulation, an examination of the model behaviour suggests we shold be looking for the following pattern (which would, I suppose, be termed a "three way interaction" if I were bothering with pointless default linear models that have no theoretical meaning here). If an individual participant is sensitive to sampling manipulation we should expect to produce something like this in their behaviour:

- In category sampling, increasing sample size increases the response to near items
- In category sampling, increasing sample size increases the response to distant items
- In property sampling, increasing sample size increases the response to near items
- In property sampling, increasing sample size *decreases* the response to distant items

When writing this, we define "near items" to be those stimuli that were included as part of the training phase (i.e., test items 1 and 2), and "distant items" to be those stimuli that fall outside the range spanned by the near items (i.e., test items 3, 4, 5 and 6).

In the actual data we might expect the data from single subjects to be relatively noisy, of course, given that we do not have large numbers of observations from each person. We can also note that we wouldn't necessarily expect the four individual components to occur with equal frequency. The model makes clear predictions about the *direction*, but it does not entail a strong claim about magnitude. For example, here is what the model behaviour looks like when we set $\tau = 3$, $\rho = 1$, $\sigma = .1$ and $\mu = .05$

```{r, fig.height=3}
sampling_frames_model(tau = 3, rho = 1, sigma = .1, mu = .05) %>% 
  model_plot()
```

At these parameters the effect for "category sampling, distant items" has attenuated to almost nothing (whereas it was quite large at the original parameter settings), and the effect for "property sampling, near items" is now quite large (though it was almost non existent at the original settings). Per the simulations reported by Hayes et al (2019), the *ordinal/directional* effects predicted by the model appear to be very robust across parameter settings, but as this illustrates the *magnitudes* are not stable across parameter values. Accordingly, when looking for "model consistent" patterns at the individual subject level we will focus primarily on the ordinal information. 

Before proceeding to consider individual subject level patterns, it is also worth considering what it would mean for a participant to be insensitive to the sampling manipulation when described in ordinal terms. This is not merely the "opposite" of the pattern listed above. In this respect, the Hayes et al (2019) model does not offer strong guidance because it is not really designed to model insensitivity. However, we can at the very least note the following as reasonable claims. If "insensitivity" holds:

- Increasing the sample size should have the same directional effect on responses to near items in the category condition and the distant items
- Increasing the sample size should have the same directional effect on responses to distant items in the category condition and the distant items

It does not necessarily mean that responses to near items and distant items should behave the same way.

## Individual subject level


```{r import-data-2, message=FALSE}
import_data <- function(experiment) {
  here::here("data", paste0(experiment, ".csv")) %>% 
    readr::read_csv() %>% 
    dplyr::mutate(
      response = response / 10,
      source = "human", 
      exp = experiment,
      id = paste0(exp, "_", id),
      stimulus = dplyr::case_when(
        test_item %in% 1:2 ~ "near", 
        test_item %in% 3:6 ~ "distant",
        TRUE ~ NA_character_
      )
    )
}

human_individual <- dplyr::bind_rows(
  import_data("exp1"), 
  import_data("exp2")
)

human_ss_effect <- human_individual %>% 
  dplyr::group_by(sampling_frame, stimulus, id, exp) %>% 
  dplyr::summarise(ss_effect = (lm(response ~ sample_size))$coef[2]) %>% 
  dplyr::ungroup()
```

A simple way to examine the data at an individual subject level is to import it with no aggregation at all, then estimate the effect of "adding one training item" on the generalisation response by fitting a linear regression separately for each participant, stimulus (near or distant) and sampling type (category or property). This will of course produce a very noisy estimate for a single person. Our interest here is looking at the distribution of these coefficients in each condition:


```{r visualise-data-2, fig.height=8}
ggplot2::ggplot(
  data = human_ss_effect %>% dplyr::mutate(
    exp = exp %>% stringr::str_replace_all("exp", "Experiment "),
    sampling_frame = sampling_frame %>% stringr::str_to_sentence(),
    stimulus = stimulus %>% 
      stringr::str_to_sentence() %>% 
      factor(levels = c("Near", "Distant")),
  ),
  mapping = ggplot2::aes(
    x = stimulus,
    y = ss_effect
  )
) + 
  ggplot2::facet_grid(
    rows = ggplot2::vars(exp),
    cols = ggplot2::vars(sampling_frame)
  ) +
  ggplot2::geom_violin(fill = "grey80") +
  ggplot2::geom_hline(
    yintercept = 0, 
    colour = "black",
    linetype = "dashed"
  ) + 
  ggplot2::geom_jitter(
    mapping = ggplot2::aes(colour = ss_effect > 0),
    show.legend = FALSE,
    width = .1,
    height = 0,
    size = 2,
    alpha = .5
  ) +
  ggplot2::geom_point(
    stat = "summary",
    fun = mean,
    size = 4
  ) +
  ggplot2::theme_bw() + 
  ggplot2::scale_x_discrete(name = "Stimulus type") + 
  ggplot2::scale_y_continuous(
    name = "Sample size effect on response",
    minor_breaks = NULL
  ) + 
  ggplot2::theme(panel.grid.major.y = ggplot2::element_blank()) + 
  ggplot2::scale_color_brewer(
    name = "Sample size", 
    palette = "Set1"
  ) 
```


Here is a quick tabulation. For each of the four conditions (cd = category distant, etc...), did the participant increase (1) or decrease (0) their judgements as a function of sample size: 

```{r}
patterns <- human_ss_effect %>% 
  dplyr::mutate(upward = ss_effect > 0) %>% 
  tidyr::unite("case", sampling_frame, stimulus) %>% 
  tidyr::pivot_wider(id_cols = c(id, exp), names_from = case, values_from = upward) %>% 
  dplyr::select(-id) %>% 
  dplyr::mutate(
    cd_cn_pd_pn = paste(
      as.numeric(category_distant), 
      as.numeric(category_near), 
      as.numeric(property_distant), 
      as.numeric(property_near), 
      sep = "_"
    )
  ) %>% 
  dplyr::select(exp, cd_cn_pd_pn) %>% 
  dplyr::group_by(exp) %>% 
  dplyr::count(cd_cn_pd_pn)

patterns %>% 
  dplyr::filter(exp == "exp1") %>%   
  dplyr::arrange(-n)

patterns %>% 
  dplyr::filter(exp == "exp2") %>%   
  dplyr::arrange(-n)
```

In this output, the "canonical" pattern predicted by the sampling frame model is 1_1_0_1, namely that the effect is positive in every condition except the property-distant. Happily for us this is the most common pattern in each case. 

What about if there were no effect of the frame manipulation? That's not quite so clear. The model suggests that we should expect that 1_1_1_1 would be the typical pattern, but that is based on the idea that "weak" sampling will push everything upwards in this task. But at a minimum we would expect conditions of the form "a_b_a_b" (i.e., always the same direction for property and category) where a and b may or may not be different


```{r}
 human_ss_effect %>% 
  dplyr::mutate(upward = ss_effect > 0) %>% 
  tidyr::unite("case", sampling_frame, stimulus) %>% 
  tidyr::pivot_wider(id_cols = c(id, exp), names_from = case, values_from = upward) %>% 
  dplyr::mutate(pattern_type = dplyr::case_when(
    category_distant == TRUE & category_near == TRUE & property_distant == FALSE & property_near == TRUE ~ "canonical_frame",
    category_distant == property_distant & category_near == property_near ~ "no_frame_effect",
    TRUE ~ "other"
  )) %>% 
  dplyr::group_by(exp) %>% 
  dplyr::count(pattern_type) 

```
These numbers should be compared to the base rates expected by chance: only 1 of 16 patterns matches the canonical frame effect 4 of 16 patterns match the no-frame patterns, and the remaining 11 of 16 fall into the "other" bin.
