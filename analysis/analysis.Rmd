---
title: "Analysis"
author: "Danielle Navarro"
date: "30-Jun-2021"
output:
  rmarkdown::html_document:
    theme: flatly
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source(here::here("models", "sampling_frames_model.R"))
`%>%` <- magrittr::`%>%`
set.seed(1234)
```

## Aggregate level

To begin, let's import the data and examine it at an aggregate level. Here's the data:

```{r import-data, message=FALSE}
ci_quantile <- function(x, quantile) {
  mean(x) + qt(quantile, length(x) - 1) * sd(x) / sqrt(length(x) - 1)
}

import_data <- function(experiment) {
  here::here("data", paste0(experiment, ".csv")) %>% 
    readr::read_csv() %>% 
    dplyr::group_by(sample_size, sampling_frame, test_item) %>% 
    dplyr::summarise(
      ci_lower = ci_quantile(response / 10, .025),
      ci_upper = ci_quantile(response / 10, .975),
      response = mean(response / 10)
    ) %>% 
    dplyr::ungroup() %>% 
    dplyr::mutate(source = "human", exp = experiment)
}

human_aggregate <- dplyr::bind_rows(import_data("exp1"), import_data("exp2"))
human_aggregate
```

Here is a simple visualisation that plots the mean and 95% confidence interval for the average response in every condition in and both experiments. (The confidence intervals here are Student intervals calculated independently for each condition)

```{r fig1-data-visualisation}
empirical_plot <- function(data_aggregate) { 
  
  ggplot2::ggplot(
    data = data_aggregate %>% dplyr::mutate(
      exp = exp %>% stringr::str_replace_all("exp", "Experiment "),
      sampling_frame = sampling_frame %>% stringr::str_to_sentence()
    ), 
    mapping = ggplot2::aes(
      x = test_item, 
      y = response,
      colour = factor(sample_size),
      group = sample_size
    )
  ) +
    ggplot2::facet_grid(
      rows = ggplot2::vars(exp),
      cols = ggplot2::vars(sampling_frame)
    ) + 
    ggplot2::geom_path(
      show.legend = FALSE,
      size = .25,
      linetype = "dashed",
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      )
    ) + 
    ggplot2::geom_pointrange(
      mapping = ggplot2::aes(
        ymin = ci_lower,
        ymax = ci_upper,
      ),
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      )
    ) +
    ggplot2::theme_bw() + 
    ggplot2::coord_cartesian(ylim = c(0, 1)) + 
    ggplot2::scale_x_continuous(
      name = "Test stimulus",
      breaks = 1:6, 
      minor_breaks = NULL
    ) + 
    ggplot2::scale_y_continuous(
      name = "Response",
      minor_breaks = NULL
    ) + 
    ggplot2::scale_color_brewer(
      name = "Sample size", 
      palette = "Dark2"
    )
}

empirical_plot(human_aggregate)
```

This visualisation oversimplifies a little as it doesn't consider systematic individual differences and reports only aggregated data, but it is helpful for illustrating what the overall pattern looks like. As a point of comparison, here is an example of the kind of behaviour one might expect from the model. In this example we use the same parameter values reported by Hayes et al (2019), though not for any principled reason because their designs were somewhat different to ours. However, those parameter settings ($\tau = 1.5$, $\rho = .1$, $\sigma = .5$ and $\mu = .5$) make a convenient starting point, and as demonstrated by the simulations reported by Hayes et al, the key qualitative predictions of the model are almost invariant across the parameter space: 

```{r fig2-model-visualisation, fig.height=3}
model_plot <- function(model) {
  ggplot2::ggplot(
    data = model %>% dplyr::mutate(
      sampling_frame = sampling_frame %>% stringr::str_to_sentence()
    ), 
    mapping = ggplot2::aes(
      x = test_item, 
      y = response,
      colour = factor(sample_size),
      group = sample_size
    )
  ) +
    ggplot2::facet_grid(
      cols = ggplot2::vars(sampling_frame)
    ) + 
    ggplot2::geom_path(
      show.legend = FALSE,
      size = .25,
      linetype = "dashed",
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      )
    ) + 
    ggplot2::geom_point(
      position = ggplot2::position_dodge2(
        width = 0.5, 
        padding = 0.5
      ),
      size = 3
    ) +
    ggplot2::theme_bw() + 
    ggplot2::coord_cartesian(ylim = c(0, 1)) + 
    ggplot2::scale_x_continuous(
      name = "Test stimulus",
      breaks = 1:6, 
      minor_breaks = NULL
    ) + 
    ggplot2::scale_y_continuous(
      name = "Response",
      minor_breaks = NULL
    ) + 
    ggplot2::scale_color_brewer(
      name = "Sample size", 
      palette = "Dark2"
    )
}

sampling_frames_model(tau = 1.5, rho = .1, sigma = .5, mu = .5) %>% 
  model_plot()
```

What matters for the moment is that the model behaviour is (qualitatively speaking) quite similar to the aggregate pattern of results produced by human participants, replicating the findings of Hayes et al (2019). Some aspects to this pattern are not theoretically interesting, however. For example, in both the property and category sampling conditions there is a tendency for generalisation responses to diminish as a function of distance (left to right in the plots). This pattern is of course captured by the model, but this is not diagnostic of anything: the relationship between similarity and generalisation is one of the oldest and most consistent effects in the literature. The fact that our data and model both reproduce it is a small sanity check that our data are not garbage, but it is otherwise of no interest to us. 

In terms of the qualitative patterns that *are* relevant to the sampling manipulation, an examination of the model behaviour suggests we shold be looking for the following pattern (which would, I suppose, be termed a "three way interaction" if I were bothering with pointless default linear models that have no theoretical meaning here). If an individual participant is sensitive to sampling manipulation we should expect to produce something like this in their behaviour:

- In category sampling, increasing sample size increases the response to near items
- In category sampling, increasing sample size increases the response to distant items
- In property sampling, increasing sample size increases the response to near items
- In property sampling, increasing sample size *decreases* the response to distant items

When writing this, we define "near items" to be those stimuli that were included as part of the training phase (i.e., test items 1 and 2), and "distant items" to be those stimuli that fall outside the range spanned by the near items (i.e., test items 3, 4, 5 and 6).

In the actual data we might expect the data from single subjects to be relatively noisy, of course, given that we do not have large numbers of observations from each person. We can also note that we wouldn't necessarily expect the four individual components to occur with equal frequency. The model makes clear predictions about the *direction*, but it does not entail a strong claim about magnitude. For example, here is what the model behaviour looks like when we set $\tau = 3$, $\rho = 1$, $\sigma = .1$ and $\mu = .05$

```{r fig3, fig.height=3}
sampling_frames_model(tau = 3, rho = 1, sigma = .1, mu = .05) %>% 
  model_plot()
```

At these parameters the effect for "category sampling, distant items" has attenuated to almost nothing (whereas it was quite large at the original parameter settings), and the effect for "property sampling, near items" is now quite large (though it was almost non existent at the original settings). Per the simulations reported by Hayes et al (2019), the *ordinal/directional* effects predicted by the model appear to be very robust across parameter settings, but as this illustrates the *magnitudes* are not stable across parameter values. Accordingly, when looking for "model consistent" patterns at the individual subject level we will focus primarily on the ordinal information. 

Before proceeding to consider individual subject level patterns, it is also worth considering what it would mean for a participant to be insensitive to the sampling manipulation when described in ordinal terms. This is not merely the "opposite" of the pattern listed above. In this respect, the Hayes et al (2019) model does not offer strong guidance because it is not really designed to model insensitivity. However, we can at the very least note the following as reasonable claims. If "insensitivity" holds:

- Increasing the sample size should have the same directional effect on responses to near items in the category condition and the distant items
- Increasing the sample size should have the same directional effect on responses to distant items in the category condition and the distant items

It does not necessarily mean that responses to near items and distant items should behave the same way.

## Individual subject level


```{r import-data-2, message=FALSE}
import_data <- function(experiment) {
  here::here("data", paste0(experiment, ".csv")) %>% 
    readr::read_csv() %>% 
    dplyr::mutate(
      response = response / 10,
      source = "human", 
      exp = experiment,
      id = paste0(exp, "_", id),
      stimulus = dplyr::case_when(
        test_item %in% 1:2 ~ "near", 
        test_item %in% 3:6 ~ "distant",
        TRUE ~ NA_character_
      )
    )
}

human_individual <- dplyr::bind_rows(
  import_data("exp1"), 
  import_data("exp2")
)
```

In order to have a measure of the "effect" of increasing sample size for a given participant in a given condition, we need to abstract slightly from the raw data because each person provides multiple relevant observations. In the simplest case, for example, here are the relevant data for participant 1 in experiment 2, for the near items in the category sampling condition:

```{r}
human_individual %>% 
  dplyr::filter(
    id == "exp2_1", 
    test_item %in% 1:2, 
    sampling_frame == "category"
  )
```

A simple way to extract an "effect of increasing sample size" measure is to run a linear regression of response against sample size and use the coefficient associated with sample size as the measure:
```{r}
human_individual %>% 
  dplyr::filter(
    id == "exp2_1", 
    test_item %in% 1:2, 
    sampling_frame == "category"
  ) %>% 
  lm(response ~ sample_size, .)
```

So for this participant the relevant coefficient is 0.06, suggesting that increasing the sample size by 1 tended to increase the generalisation response by 0.06 on average for this person. (There is, of course, no point in testing for significance here - the purpose of this linear model is to extract a point estimate only for the purposes of summarising the observations. Though I have relied on a statistical model, this is a descriptive statistic for all intents and purposes)


```{r estimate-ss-effect}
human_ss_effect <- human_individual %>% 
  dplyr::group_by(sampling_frame, stimulus, id, exp) %>% 
  dplyr::summarise(ss_effect = (lm(response ~ sample_size))$coef[2]) %>% 
  dplyr::ungroup()
```

In the figure below, I've plotted the empirical distribution of these coefficients in each condition and each experiment. Each coloured dot shows the coefficient for a single participant, and the colour is used to indicate the sign: positive valued coefficients are coloured in blue, and negative valued coefficients are coloured in red. The grey violin plots shown behind them depict gaussian kernel density estimates of the distribution (truncated to the range of the data, using a bandwidth of .01 to ensure fairly smooth densities). Large black dots show the mean coefficient in each condition:

```{r fig4-visualise-data-2, fig.height=8}
ggplot2::ggplot(
  data = human_ss_effect %>% dplyr::mutate(
    exp = exp %>% stringr::str_replace_all("exp", "Experiment "),
    sampling_frame = sampling_frame %>% stringr::str_to_sentence(),
    stimulus = stimulus %>% 
      stringr::str_to_sentence() %>% 
      factor(levels = c("Near", "Distant")),
  ),
  mapping = ggplot2::aes(
    x = stimulus,
    y = ss_effect
  )
) + 
  ggplot2::facet_grid(
    rows = ggplot2::vars(exp),
    cols = ggplot2::vars(sampling_frame)
  ) +
  ggplot2::geom_violin(fill = "grey80", bw = .01) +
  ggplot2::geom_hline(
    yintercept = 0, 
    colour = "black",
    linetype = "dashed"
  ) + 
  ggplot2::geom_jitter(
    mapping = ggplot2::aes(colour = ss_effect > 0),
    show.legend = FALSE,
    width = .1,
    height = 0,
    size = 2,
    alpha = .5
  ) +
  ggplot2::geom_point(
    stat = "summary",
    fun = mean,
    size = 4
  ) +
  ggplot2::theme_bw() + 
  ggplot2::scale_x_discrete(name = "Stimulus type") + 
  ggplot2::scale_y_continuous(
    name = "Sample size effect on response",
    minor_breaks = NULL
  ) + 
  ggplot2::theme(panel.grid.major.y = ggplot2::element_blank()) + 
  ggplot2::scale_color_brewer(
    name = "Sample size", 
    palette = "Set1"
  ) 
```

The tabulation below counts the number of participants whose responses shift upwards with sample size (`n_positive`) or downwards with sample size (`n_negative`). In every case, the majority of participants shift in the direction predicted by the GP model (i.e., negative for the distant items in property sampling, positive in every other case...)

Personally, I hate running more statistical tests than I absolutely need to, but I know other people care about such things so I have also added a column showing the p-value if we ran a binomial test on these data, against the null hypothesis that the true probability of an upward shift is 0.5. Unsurprisingly, most of these differences are individually significant at the standard $\alpha = .05$ level. It's pretty clear that a majority of individual subjects are reproducing the ordinal patterns predicted by the Hayes et al model...

```{r tabulate-directions, message=FALSE}
binom_tests <- function(x1, x2) {
  purrr::map2_dbl(x1, x2, function(x1, x2) {
    binom.test(
      x = x1,
      n = x1 + x2,
      p = 0.5
    )$p.value
  })
}

use_pretty_p <- function(p) {
  (scales::pvalue_format())(p)
}

human_ss_effect %>% 
  dplyr::group_by(exp, sampling_frame, stimulus) %>%
  dplyr::summarise(
    n_positive = sum(ss_effect > 0),
    n_negative = sum(ss_effect < 0),
    n_zero = sum(ss_effect == 0), 
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    p_value = binom_tests(n_positive, n_negative) %>% 
      use_pretty_p()
  )
```

The preceding analysis is fine as far as it goes, but it has a substantial limitation: it examines each condition separately. The Hayes et al model makes a much more specific prediction about individual subjects. For each person we have ordinal results for four conditions -- category-near (cn), category-distant (cd), property-near (pn) and property-distant (pd) -- and if the model was capturing something sensible at an individual subject level we should see a disproportionate number of people meeting the ordinal prediction for all four conditions. Let `+` denote a positive directional shift, `-` denote a negative directional shift and `.` denote no change. If we disregard the `.` cases on the grounds that they are difficult to interpret, the pattern we're expecting to see from people who are sensitive to the frame manipulation is `cn+ cd+ pn+ pd-` (or to use a more compact notion: `+++-`). For people who are insensitive to sampling we expect to see one of the following four patterns: `++++`, `+-+-`, `-+-+` or `----` (i.e., those in which the effect is in the same direction for category and property sampling conditions). The remaining 11 patterns of the $2^4 = 16$ possibilities should be less common. 

Here is a tabulation showing the frequency of the observed patterns:

```{r detect-patterns}
compact_sign <- function(x) {
  x %>% 
    stringr::str_replace_all("-1", "-") %>% 
    stringr::str_replace_all("1", "+") %>% 
    stringr::str_replace_all("0", ".")
}

patterns_individual <- human_ss_effect %>% 
  dplyr::mutate(upward = sign(ss_effect)) %>% 
  tidyr::unite("case", sampling_frame, stimulus) %>% 
  tidyr::pivot_wider(id_cols = c(id, exp), names_from = case, values_from = upward) %>% 
  dplyr::mutate(
    cn_cd_pn_pd = paste(
      compact_sign(category_near),
      compact_sign(category_distant),
      compact_sign(property_near),
      compact_sign(property_distant),
      sep = ""
    )
  ) %>% 
  dplyr::select(exp, cn_cd_pn_pd, id) 

patterns <- patterns_individual%>% 
  dplyr::select(exp, cn_cd_pn_pd) %>% 
  dplyr::filter(
    cn_cd_pn_pd %>% 
      stringr::str_detect(stringr::fixed("."), negate = TRUE)
  ) %>% 
  dplyr::group_by(exp) %>% 
  dplyr::count(cn_cd_pn_pd)

patterns %>% 
  dplyr::filter(exp == "exp1") %>%   
  dplyr::arrange(-n)

patterns %>% 
  dplyr::filter(exp == "exp2") %>%   
  dplyr::arrange(-n)
```

In both cases, the most common pattern is the one the model predicts for a participant who is sensitive to sampling frame in the theoretically-predicted way (i.e., `+++-`) and the four patterns that suggest "pure" insensitivity (i.e., `++++`, `+-+-`, `-+-+` or `----`) seem to be more common than you would expect under a simple null hypothesis in which all 16 patterns were equally likely. The tabulation below summarises this:

```{r classify-patterns, message=FALSE}
pattern_classes <- patterns %>% 
  dplyr::mutate(pattern_type = dplyr::case_when(
    cn_cd_pn_pd == "+++-" ~ "sensitive",
    cn_cd_pn_pd %in% c("++++", "+-+-", "-+-+", "----") ~ "insensitive",
    TRUE ~ "other"
  )) %>% 
  dplyr::group_by(exp, pattern_type) %>% 
  dplyr::summarise(n = sum(n)) %>% 
  dplyr::mutate(H0_base_rate = dplyr::case_when(
    pattern_type == "sensitive" ~ 1/16,
    pattern_type == "insensitive" ~ 4/16,
    pattern_type == "other" ~ 11/16,
    TRUE ~ NA_real_
  ))

pattern_classes
```

```{r}
# convenience for inline code below
sensitive_1   <- pattern_classes %>% dplyr::filter(pattern_type == "sensitive",   exp == "exp1") %>% dplyr::pull(n)
insensitive_1 <- pattern_classes %>% dplyr::filter(pattern_type == "insensitive", exp == "exp1") %>% dplyr::pull(n)
other_1       <- pattern_classes %>% dplyr::filter(pattern_type == "other",       exp == "exp1") %>% dplyr::pull(n)

sensitive_2   <- pattern_classes %>% dplyr::filter(pattern_type == "sensitive",   exp == "exp2") %>% dplyr::pull(n)
insensitive_2 <- pattern_classes %>% dplyr::filter(pattern_type == "insensitive", exp == "exp2") %>% dplyr::pull(n)
other_2       <- pattern_classes %>% dplyr::filter(pattern_type == "other",       exp == "exp2") %>% dplyr::pull(n)

tot_1 <- sensitive_1 + insensitive_1 + other_1      
tot_2 <- sensitive_2 + insensitive_2 + other_2

sensitive_H0 <- 1/16
insensitive_H0 <- 4/16
other_H0 <- 11/16

binom_pvalue <- function(x, n, p) {
  (binom.test(x, n, p))$p.value %>% 
    use_pretty_p()
}
```

Again I could run some generic hypothesis tests (e.g., chi-square goodness-of-fit) but I'd prefer to pick out specific comparisons that have some direct relevance to the subject matter at hand. First, note that yes, the "sensitive" pattern occurs disproportionately often:

- In experiment 1, we observe the "sensitive" pattern for `r sensitive_1` of the `r tot_1` cases. Under the null hypothesis this should occur with probability `r sensitive_H0`, whereas the sample rate is `r signif(sensitive_1 / tot_1, digits = 3)`. Binomial test p-value: `r binom_pvalue(sensitive_1, tot_1, sensitive_H0)`

- In experiment 2, we observe the "sensitive" pattern for `r sensitive_2` of the `r tot_2` cases. Under the null hypothesis this should occur with probability `r sensitive_H0`, whereas the sample rate is `r signif(sensitive_2 / tot_2, digits = 3)`. Binomial test p-value: `r binom_pvalue(sensitive_2, tot_2, sensitive_H0)`

This result is unambiguous!

Next, notice that the four "insensitive" patterns are disproportionately common in experiment 1 (though the effect, if it exists, may not very large), but not experiment 2.

- In experiment 1, we observe one of the four "insensitive" patterns for `r insensitive_1` of the `r tot_1` cases. Under the null hypothesis this should occur with probability `r insensitive_H0`, whereas the sample rate is `r signif(insensitive_1 / tot_1, digits = 3)`. Binomial test p-value: `r binom_pvalue(insensitive_1, tot_1, insensitive_H0)`

- In experiment 2, we observe one of the four "insensitive" patterns for `r sensitive_2` of the `r tot_2` cases. Under the null hypothesis this should occur with probability `r insensitive_H0`, whereas the sample rate is `r signif(insensitive_2 / tot_2, digits = 3)`. Binomial test p-value: `r binom_pvalue(sensitive_2, tot_2, insensitive_H0)`

It is possible that by we could "bump" these to significance by testing the rates of the "insensitive" patterns only to the "other" patterns. The idea here is that the "sensitive" patterns are so common relative to their supposed base rate that they're distorting the statistics for the "insensitive" patterns. Or, to put it in slightly different terms, the tests shown above aren't independent of one another. So if we *do* adopt this alternative approach and compare "insensitive" to other, here's what we get:

- In experiment 1, we observe one of the four "insensitive" patterns for `r insensitive_1` of the `r insensitive_1 + other_1` cases that are not the "sensitive" pattern. Under the null hypothesis this should occur with probability `r insensitive_H0/(insensitive_H0 + other_H0)`, whereas the sample rate is `r signif(insensitive_1 / (insensitive_1 + other_1), digits = 3)`. Binomial test p-value: `r binom_pvalue(insensitive_1, insensitive_1 + other_1, insensitive_H0/(insensitive_H0 + other_H0))`

- In experiment 2, we observe one of the four "insensitive" patterns for `r insensitive_2` of the `r insensitive_2 + other_2` cases that are not the "sensitive" pattern. Under the null hypothesis this should occur with probability `r insensitive_H0/(insensitive_H0 + other_H0)`, whereas the sample rate is `r signif(insensitive_2 / (insensitive_2 + other_2), digits = 3)`. Binomial test p-value: `r binom_pvalue(insensitive_2, insensitive_2 + other_2, insensitive_H0/(insensitive_H0 + other_H0))`

Under this formulation of the hypothesis, the result for experiment 1 is now significant at a much more stringent level, but the result for experiment 2 remains non-significant. Looking at both formulations together (which both seem reasonable) I'd conclude that there's very little evidence to think that those four "insensitive" patterns occur with disproportionate frequency. 

## Subgroup plots

The preceding analysis suggests that we have some reasonable basis for claiming that the "sensitive" pattern is unusually prominent in the data, accounting for 30-40% of participants despite being such a specific pattern. It is of course the case that this classification is a little crude and likely quite noisy, since it is based on a binary classification of the slope of a regression line for each participant. Even so, it seems a reasonable criterion for extracting two subgroups: those that show the "sensitive" pattern `+++-`, and those that show some other pattern. However, for this analysis we will need to make a decision about how to classify participants whose data contain at least one non-directional result. Does `+++.` count as sensitive or not-sensitive, for example? For the purposes of this analysis, I'll classify participants based on whether their non-`.` results match the sensitive pattern. This means that `+++.` and `++.-` would both be counted as sensitive, but `+-.-` would count as non-sensitive. The only exception is the `....` case, for which we have no evidence at all, and I'll exclude those cases:

```{r, message = FALSE}
subgroup_individual <- human_individual %>% 
  dplyr::left_join(patterns_individual, by = c("id", "exp")) %>% 
  dplyr::mutate(pattern_type = dplyr::case_when(
    cn_cd_pn_pd == "...." ~ "....",
    stringr::str_detect(cn_cd_pn_pd, "[+.][+.][+.][-.]") ~ "sensitive",
    TRUE ~ "not-sensitive"
  ))

subgroup_aggregate <- subgroup_individual %>% 
  dplyr::filter(pattern_type != "....") %>% 
  dplyr::group_by(sample_size, sampling_frame, test_item, exp, pattern_type) %>%
  dplyr::summarise(
    ci_lower = ci_quantile(response, .025),
    ci_upper = ci_quantile(response, .975),
    response = mean(response)
  ) %>% 
  dplyr::ungroup()
```

Here are the aggregated plots for the "sensitive" group:

```{r fig5}
subgroup_aggregate %>% 
  dplyr::filter(pattern_type == "sensitive") %>% 
  empirical_plot()
```

This pattern is (unsurprisingly, since this was the basis of the selection) more or less exactly what the model predicts. When we look at the aggregated data for everyone else, we see data in which the effect of sample size is almost entirely non-existent. The responses increase with sample size for the near items (test items 1 and 2), but do not appear to be very different in the category sampling versus property sampling conditions. For the distant items, there are essentially no effects at all (or miniscule ones):


```{r fig6}
subgroup_aggregate %>% 
  dplyr::filter(pattern_type == "not-sensitive") %>% 
  empirical_plot()
```


## Overall conclusions

- Some people (maybe about a third???) are sensitive to the sampling manipulation in the precise pattern we expected to see. This proportion is much higher than we would expect by chance: the model is capturing a pattern of behaviour that is fairly common

- The sensitivity doesn't seem to be universal, and among people who aren't sensitive, there is really not a lot of evidence to suggest an effect of frame or sample size (at least not for the distant/extrapolation items). For these people, the model as formulated by Hayes et al doesn't seem appropriate

- Although crude, this classification system does seem somewhat warranted by the data. This is of some value for experiment 2, since we can compare participants on working memory capacity etc based on this categorisation. Here are the participants and the classifications I constructed:

```{r}
subgroup_classification <- subgroup_individual %>% 
  dplyr::group_by(id) %>% 
  dplyr::summarise(
    age = dplyr::first(age),
    gender = dplyr::first(gender),
    cn_cd_pn_pd = dplyr::first(cn_cd_pn_pd),
    pattern_type = dplyr::first(pattern_type)
  ) %>% 
  tidyr::separate(id, c("exp", "id")) %>% 
  dplyr::mutate(id = as.numeric(id)) %>% 
  dplyr::arrange(id) 

readr::write_csv(
  subgroup_classification,
  here::here("analysis", "subgroup_classification.csv")
)

subgroup_classification
```

The complete table for both experiments is saved in the "analysis" folder of the repository, as "subgroup_classification.csv"

## Session info

```{r, echo=TRUE}
devtools::session_info()
```

