% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  doc]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Appendix B: Model description},
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\makeatother
\shorttitle{Model description}
\author{Danielle Navarro\textsuperscript{1, 2}}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} School of Psychology, UNSW Sydney\\\textsuperscript{2} UNSW Data Science Hub, UNSW Sydney}
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{bm}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[shorthands=off,main=english]{babel}
\fi

\title{Appendix B: Model description}

\date{}

\begin{document}
\maketitle

\noindent
The model used in this paper is a simplified version of the one described by Hayes et al (2019; see their Appendix B and \url{https://github.com/djnavarro/samplingframes}). The motivation for the model is to consider how one might learn an arbitrary function \(f\) defined over some stimulus space \(\mathcal{X}\). In the general case \(\mathcal{X}\) can be richly structured, but in the current setting we can consider stimuli that vary
only on a single continuous dimension.

\hypertarget{the-prior-distribution}{%
\subsection{The prior distribution}\label{the-prior-distribution}}

\noindent
The Gaussian process (GP; see Rasmussen \& Williams 2006) provides a method
for specifying priors over smooth functions \(f : \mathbb{R} \rightarrow \mathbb{R}\) that maps every possible stimulus \(x\) onto a subjective inductive strength \(y = f(x)\). The function \(f\) is defined over the entire stimulus space \(\mathcal{X}\) but is measured only at a known, finite set of points \(\bm{x} = (x_1, \ldots, x_n)\). In this setting the joint distribution over the corresponding outputs \(\bm{y} = (y_1, \ldots, y_n)\) is a finite-dimensional marginalisation of the Gaussian process and is thus a multivariate Gaussian with mean vector \(\bm{\mu} = (\mu_1, \ldots, \mu_n)\) and covariance matrix \(\bm{\Sigma} = [\sigma_{ij}]\) for \(i, j \in 1, \ldots, n\)

\[
f(x) \sim \mbox{Normal}(\bm\mu, \bm\Sigma)
\]

\noindent
The covariance matrix is used to control the smoothness of the inferred function \(f\) and defined by the kernel function \(\sigma_{ij} = K(x_i, x_j)\). Following the logic outlined by Hayes et al, we apply a radial basis kernel function kernel in which the correlation diminishes as a Gaussian function of the distance \(d_{ij}\) between items in psychological space:

\[
K(x_i, x_j) = \tau^2 \exp\left(-\rho {d_{ij}}^2\right)
\]
and
\[
\sigma_{ij} = \left\{ \begin{array}{rl} K(x_i, x_j) & \mbox{ if } i \neq j \\ K(x_i, x_i) + \sigma^2 & \mbox{ if } i = j \end{array}  \right.
\]
\noindent
This prior distribution is described by four parameters:

\begin{itemize}
\tightlist
\item
  \(\tau\) describes the baseline correlation between stimuli
\item
  \(\rho\) governs the rate at which the correlation decays with psychological distance
\item
  \(\sigma\) is the inherent noise in the data
\item
  \(\mu\) is the prior mean, where we assume that \(\mu_i = \mu\) for all \(i\)
\end{itemize}

\hypertarget{the-likelihood-function}{%
\subsection{The likelihood function}\label{the-likelihood-function}}

\noindent
We consider three different sampling assumptions the learner might make. Suppose the learner has observed a stimulus \(x\) that is property-positive (i.e., possesses the property).

\begin{itemize}
\item
  Under \emph{property sampling} the learner assumes that this item was chosen uniformly at random from the set of property-positive stimuli. If \(\phi_i\) denotes the probability (or proportion) of \(x_i\) items (e.g., rocks of this specific size) that are plaxium positive, and (for simplicity) assume that the base rates of all stimulus types are the same, then the probability that a random plaxium-positive stimulus will turn out to have value \(x_i\) is
  \[
  P(x_i | \bm{\phi}) = \frac{\phi_i}{\sum_j \phi_j}
  \]
  where the sum in the denominator is taken across all possible stimuli (for these designs that's just the six test items).
\item
  Under \textbf{category sampling} the items are sampled uniformly at random from items that belong to the category (in this case, the category consists of 2 of the 6 test items only). If there are \(|c|\) items in the category that have the same base rates, then the probability of the stimulus value \(x_i\) is \(1/|c|\). The probability that the item is also property positive depends on the value of \(\phi_i\) and so
  \[
  P(x_i | \bm{\phi}) = \frac{\phi_i}{|c|}
  \]
\item
  Under \textbf{weak sampling}, items are sampled uniformly at random from all stimuli included in the experiment. In other words, it's identical to category sampling except that the \enquote{category} is larger. If \(|x|\) denotes the total number of items (in this case 6)

  \[
  P(x_i | \bm{\phi}) = \frac{\phi_i}{|x|}
  \]
  Because they differ only by a constant that disappears later during the normalisation weak sampling and category sampling are indistinguishable for this design.
\end{itemize}

\noindent
Elsewhere in the literature, it has been argued that weak sampling is essentially the model one assumes when one has no theory of the task. Navarro et al (2012) formalised this idea slightly by arguing that people might adopt a \enquote{mixed} sampling model where items are strongly sampled (be that property or category) with probability \(\theta\), and weakly sampled with probability \(1-\theta\). The version of the model we implement includes this \(\theta\) parameter: however, because weak and category sampling are identical in this design, the only effect that \(\theta\) has is to change model behaviour in the property sampling condition. Setting \(\theta = 0\) produces a null effect of sampling frame (i.e., model predicts no difference between category and property sampling, and that both conditions should look like weak/category sampling), whereas \(\theta = 1\) produces the largest possible effect.

\hypertarget{generalisation-gradients}{%
\subsubsection{Generalisation gradients}\label{generalisation-gradients}}

\noindent
Suppose the learner has seen items \(\bm x\) that are property-positive. Bayesian reasoning yields:

\[ 
P({\bm \phi}_h | \bm x) = \frac{P(\bm x | \bm{\phi}_h) P(\bm{\phi}_h)}{\int P(\bm x | \bm{\phi}_h) P(\bm{\phi}_h) \ dh^\prime}
\]

\noindent
As noted below, we approximate the full hypothesis space \(\mathcal{H}\) of possible functions by a finite set so

\[ 
P({\bm \phi}_h | \bm x) \approx \frac{P(\bm x | \bm{\phi}_h) P(\bm{\phi}_h)}{\sum_{h^\prime} P(\bm x | \bm{\phi}_h) P(\bm{\phi}_h)}
\]
The generalisation probability is obtained by computing the expected probability that a stimulus located at \(y_i\) is plaxium positive:

\[
g(y_i | \bm{x}) = \sum_{h^\prime} \phi_{ih} P({\bm \phi}_h | \bm x)
\]

\hypertarget{implementation-details}{%
\subsection{Implementation details}\label{implementation-details}}

Implemented in R, used finite hypotheses space consisting of \(5^6 = 15625\) functions. Link to source code. Etc. Default parameter values.

\hypertarget{references}{%
\subsection{References}\label{references}}

\begin{itemize}
\tightlist
\item
  Hayes, B. K., Banner, S., Forrester, S. and Navarro, D. J. (2019). Selective sampling and inductive inference: Drawing inferences based on observed and missing evidence. \emph{Cognitive Psychology, 113}. \url{https://doi.org/10.1016/j.cogpsych.2019.05.003}
\item
  Navarro, D. J., Dry, M. J. and Lee, M D. (2012). Sampling assumptions in inductive generalization. \emph{Cognitive Science, 36}, 187-223. \url{https://dx.doi.org/10.1111/j.1551-6709.2011.01212.x}
\item
  Rasmussen, C. E., and Williams, C. K. I. (2006). \emph{Gaussian Processes for Machine Learning}
\end{itemize}

\end{document}
